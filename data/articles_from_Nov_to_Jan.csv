introductions,month,year
"['Let consider the following scenario: you have implemented an outstanding machine learning model that predicts if a patient is suffering or not from paludism. Then hospitals in your city want to integrate your model into their systems for general use. But those systems have been developed in completely different programming languages. Does it mean that they won’t be able to use your model anymore? The answer is No because, with RESTful services, your model can be made available to application developers, no matter which programming language they use. This article has two sections: Build a machine learning model that predicts if a given patient has diabetes or not. Walkthrough the steps to REST-enable your machine learning model with Django REST APIs.', 'Getting data comes as the second step in any data science/machine learning project lifecycle, right after framing the problem you want to solve, which would make this step be the backbone of the rest of the phases. Also, social media are great places to collect data, especially for competitor analysis, topic research, sentiment analysis, etc. This article aims to perform a step-by-step implementation on how to get credentials and the implementation on a simple use case.', 'Machine learning experimentation results are challenging to reproduce. Without detailed tracking, the same results might not be reproducible from one team to another. In this article, we will first discuss the challenges related to developing end-to-end machine learning systems, and some suggestions of tools by some big corporations to mitigate them. Secondly, we will identify the limitations of those tools, and how mlflow tackles them. To finish, we will have a concrete use case in order to have some in-depth understanding of mlflow.', 'As Machine Learning or Software Engineer, we would like to share our applications/models and their dependencies with others. This process, if not done properly can cost us not only time but also money. Nowadays, one of the most preferred ways to collaborate with other developers is by using source code management systems (e.g. Git). This might not be enough, because the awesome model that you developed on your computer might not work on someone else’s computer for different reasons. So a better way to tackle this issue might be to use Docker containers, in order to be able to “replicate” your computer and make it accessible to others. In this article, we will learn this concept in the following order: Train a simple sentiment classifier Create the corresponding Streamlit application using good software project structuring. Create the Docker image for streamlit application. Run the image in a container']",november,2021
"['We are living in an era where we are surrounded by a large volume of textual information such as survey responses, social media comments, tweets, etc. Finding suitable information for one’s needs can be challenging, especially when dealing with a large yet different corpus of data. Thanks to topic modeling, an era of natural language processing used to efficiently analyze big unlabeled text data by grouping/clustering them into topics. We will try to cover a technic of topic modeling called LDA with python.', 'Pandas stands out among the data analysis tools for data scientists mainly working with Python, because of both its ease of use and the flexibility it offers for data processing. Despite all these benefits, it is unfortunately not as efficient when it comes to manipulating very large datasets. There are tools to overcome this challenge. Here comes Datatable, a python library for processing tabular data. It is one of the most efficient tools, supporting out-of-memory datasets, multi-threaded data processing, and flexible API. In this article, we will introduce some basics of Datatable and a comparative analysis with Pandas.', 'As Data Scientist or Machine Learning practitioner, integrating an explainability layer into your machine learning models can make them trustworthy. It can help decision-makers and other stakeholders have more visibility and understand explanations of the decisions that led to the models’ output.In this article, I will walk you through two surrogate models, LIME and SHAP, to help you understand the decision-making process of your models.', 'As Data Scientists, we are led to exploit as much as possible the data sources available within or external to organizations in order to respond in the most relevant way to their problems. These data can be of different formats and sometimes difficult to handle. This article mainly focuses on two main aspects: text data extraction and tabular data extraction. The list of libraries is not exhaustive, the goal is to focus on 5 of them, with 3 for text data extraction and 2 for tabular data extraction. Additional information can be found at the end of the article.']",december,2021
"['Data is available in different formats such as word, pdf, etc. Sometimes you might want to collect the content of those files in an efficient manner. But how can you perform such a task without dealing with hundreds of tools? Here comes the Apache Tika, a tool that can extract metadata and text from over a thousand different files types. All these extractions can be performed using a simple uniform API. This article focuses on walking through the process of understanding the format of tika output and also the content extraction', 'Managing information from the financial market is not an easy task, especially in today’s fast and complex ecosystem. Financial actors might want to predict the change in stock price or identify companies that better fit their investment requirements; those are some of the challenges amongst a plethora of challenges that financial actors have been trying to tackle. However, using previous financial text information, actors can increase their chance of finding the best company to invest in or predict the right change in stock price. This article will try to use deep learning in order to classify is given financial news headlines using FinBERT.', 'When reading a text, a human being can naturally identify named entities like dates, currencies, locations, person names, medical codes, brands, etc. This step can then be relevant for further information extraction from large text data in order to better answer questions such as: what is the name of the covid19 variant trending on the news? what tools are mentioned in a given job description and at what level of proficiency? etc. The goal of this article is to use spaCy and roBERTa from spaCy transformers in order to automatically identify and extract entities as defined early in the introduction.', 'We are living in an era where we are surrounded by a large volume of textual information such as survey responses, social media comments, tweets, etc. Finding suitable information for one’s needs can be challenging, especially when dealing with a large yet different corpus of data. Thanks to topic modeling, an era of natural language processing used to efficiently analyze big unlabeled text data by grouping/clustering them into topics. Recently, I came across BERTopic, a straightforward yet powerful library for topic modeling. So, I decided to give it a try, and the results are powerful!When reading an interesting article, you might want to find similar articles from a large corpus of data. Manual processing is obviously not the strategy to go for. So, why not take advantage of the power of Artificial Intelligence to solve such problems? From this article, you will be able to use SciBERT, and two different similarity approaches (Cosine and k-NN) in order to find scientific articles that are most similar in meaning to your specific query.']",january,2022
